{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16124079",
   "metadata": {},
   "source": [
    "# Phase II - Introduction to Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8c212e",
   "metadata": {},
   "source": [
    "1. Data organization: it is the process of structuring data for analysis and interpretation.\n",
    "\n",
    "   1. Tabular data: organizing data in rows and columns.\n",
    "   2. Frequency distribution: organizing data into categories based on their values.\n",
    "   3. Time series: organizing data over time.\n",
    "   4. Geographic data: organizing data based on location.\n",
    "   5. Categorical data: organizing data into categories.\n",
    "   6. Hierarchical data: organizing data in a tree-like structure.\n",
    "   7. Network data: organizing data as a graph.\n",
    "   8. Hierarchical data: organizing data in a tree-like structure.\n",
    "\n",
    "2. Data Visualization: it involves representing data in graphical formats to communicate insights effectively.\n",
    "   1. Charts and graphs: using visual representations such as bar charts, line graphs, and scatter plots to illustrate data trends and patterns.\n",
    "   2. Shapes of data distributions: visualizing the shape of data to understand central tendency (normal distribution) and variability. You can use histograms, box plots, and density plots to visualize the distribution of data. ANother method is Shapiro -Wilk test for normality (in Python scipy.stats.shapiro) and  Q-Q plots (in Python matplotlib.pyplot.qqplot) and visualizing data with Q-Q plots (in Python matplotlib.pyplot.qqplot) for assessing normality. Other techniques include using skewness and kurtosis (in python scipy.stats.skew) and kurtosis (in Python scipy.stats.kurtosis)to assess distribution shape. One more test is the Kolmogorov-Smirnov test for normality (in Python scipy.stats.kstest).\n",
    "      1. Shapiro-wilk test is a statistical test used to determine whether a sample comes from a normally distributed population. It assesses the null hypothesis that the data follows a normal distribution, comparing the observed distribution of the sample to a normal distribution. The test involves calculating a test statistic and comparing it to a critical value based on the sample size and significance level. If the test statistic is less than the critical value, the null hypothesis is rejected, indicating that the data does not follow a normal distribution. The Shapiro-Wilk test is commonly used in statistical analysis to assess the normality of data.\n",
    "      2. Another method for assessing normality is the Anderson-Darling test (in Python, scipy.stats.anderson), which evaluates how well a sample fits a specified distribution. It provides a test statistic and critical values for different significance levels, allowing researchers to determine if the data deviates from normality. The Anderson-Darling test is useful for detecting deviations from normality in a sample.\n",
    "      3. The Kolmogorov-Smirnov this test is a non-parametric test used to compare a sample distribution to a reference probability distribution, such as the normal distribution. It assesses whether a sample comes from a specific distribution by comparing the empirical distribution function of the sample to the cumulative distribution function of the reference distribution. The test statistic is the maximum absolute difference between the empirical distribution function and the reference distribution function. If the test statistic is small, it suggests that the sample comes from the reference distribution. The Kolmogorov-Smirnov test is particularly useful for assessing the goodness of fit between the sample and the reference distribution, allowing researchers to make informed decisions about the underlying distribution of the data.\n",
    "\n",
    "3. Data summary: it provides a concise representation of data characteristics, often using measures like mean, median, and mode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4711887",
   "metadata": {},
   "source": [
    "## Shapes of Distributions\n",
    "\n",
    "In statistics, the shape of a distribution describes how the values are spread or clustered around the mean. Common shapes include normal, skewed, uniform, and bimodal distributions. Understanding these shapes helps in analyzing data and making predictions.\n",
    "\n",
    "1. Normal distributions are symmetric and bell-shaped, indicating that data points are evenly distributed around the mean.\n",
    "2. Skewed distributions have a tail on one side, indicating that data points are not evenly distributed. A right-skewed distribution has a longer tail on the right, while a left-skewed distribution has a longer tail on the left.\n",
    "3. Uniform distributions have all values equally likely, resulting in a flat shape, indicating no clustering around the mean.\n",
    "4. Bimodal distributions have two distinct peaks, indicating that the data points cluster around two different values.\n",
    "5. Multimodal distributions, similar to bimodal, have multiple peaks, indicating that the data points cluster around several different values, which can suggest the presence of distinct subgroups within the data.\n",
    "6. Exponential distributions have a rapid decay, indicating that data points cluster near the start and decrease quickly, often used to model time until an event occurs.\n",
    "7. Log-normal distributions are skewed and characterized by a distribution of the logarithm of the variable being normally distributed, indicating that data points are clustered around a certain value but can extend to higher values.\n",
    "8. Pareto distributions, also known as power-law distributions, are characterized by a small number of occurrences with high values and a large number of occurrences with low values, often used to describe phenomena such as wealth distribution.\n",
    "9. J distibutions, also known as Zipf's law, exhibit a relationship where a few items are very common while many items are rare, often seen in natural and social phenomena.\n",
    "10. Reverse J distributions, also known as Zipf's law, exhibit a relationship where a few items are rare while many items are common, often seen in natural and social phenomena.\n",
    "11. Log-logistic distributions are characterized by a long tail to the right, indicating that data points are clustered near the start and extend to higher values.\n",
    "12. Triangular distributions are characterized by a peak at the mean and a long tail on both sides, indicating that data points are clustered around the mean but can extend to higher values.\n",
    "13. Beta distributions are characterized by a distribution of the variable being normally distributed, indicating that data points are clustered around a certain value but can extend to higher values.\n",
    "14. Gamma distributions are characterized by a distribution of the variable being normally distributed, indicating that data points are clustered around lower values but can extend to higher values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d5684",
   "metadata": {},
   "source": [
    "## Parameter vs Statistics\n",
    "\n",
    "Population -----> Sample ------> Statistic [Descriptive Statistics] ------> Parameter [Inference Statistics]------> Population\n",
    "\n",
    "\n",
    "Sample Statistic -----> Estimate ------> Hypothesis Test\\\n",
    "Sample Statistic -----> Confidence Interval ------> Regression Analysis\\\n",
    "Sample Statistic -----> p-value ------> Effect Size\\\n",
    "Sample Statistic -----> Model -----> Model Validation\\\n",
    "Sample Statistic -----> Variance ------> Standard Deviation\\\n",
    "Sample Statistic -----> Correlation ------> Covariance\\\n",
    "Sample Statistic -----> Outlier Detection ------> Data Cleaning\\\n",
    "Sample Statistic -----> Normality Test ------> Data Transformation\\\n",
    "Sample Statistic -----> Multicollinearity Test ------> Data\\ \n",
    "Sample Statistic -----> Homoscedasticity Test ------> Data Interpretation\n",
    "\n",
    "Assumptions Testing\n",
    "\n",
    "Sample Statistic -----> ANOVA ------> Statistical Analysis\\\n",
    "Sample Statistic -----> Regression Analysis ------> Model Evaluation\\\n",
    "Sample Statistic -----> Time Series Analysis ------> Forecasting\\\n",
    "Sample Statistic -----> Chi-Squared Test ------> Categorical Data Analysis\\\n",
    "Sample Statistic -----> Bayesian Analysis ------> Predictive Modeling\\\n",
    "Sample Statistic -----> Factor Analysis ------> Dimensionality Reduction\\\n",
    "Sample Statistic -----> Data Visualization ------> Insights Generation\\\n",
    "Sample Statistic -----> Data Mining ------> Knowledge Discovery\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc4ffac",
   "metadata": {},
   "source": [
    "# Measure of Dispersion\n",
    "\n",
    "In statistics, dispersion refers to the extent to which a distribution is stretched or squeezed. Common measures of dispersion include range, variance, and standard deviation.\n",
    "1. Absolute Measures: Range, variance, standard deviation and interquartile range (IQR) are examples of absolute measures that describe the spread of data points in a dataset.\n",
    "   \n",
    "2. Relative Measures: Coefficient of variation, coeffient of quartile deviation, coefficient of mean deviation are examples of relative measures that express dispersion in relation to the mean of the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
