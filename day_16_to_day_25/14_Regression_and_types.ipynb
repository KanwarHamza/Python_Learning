{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6696f2a",
   "metadata": {},
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5674cfef",
   "metadata": {},
   "source": [
    "Key points about Linear Regression:\n",
    "\n",
    "    It is a simple and commonly used machine learning algorithm for predicting continuous or numeric outcomes.\n",
    "\n",
    "    It works by finding a linear relationship between two or more variables – a dependent variable and one or more independent variables.\n",
    "\n",
    "    The relationship is modeled using a straight line or linear equation of the form y = mx + c, where m is the slope and c is the y-intercept.\n",
    "\n",
    "    It assumes a linear relationship between dependent and independent variables and finds the coefficients that minimize the prediction errors using techniques like Gradient Descent.\n",
    "\n",
    "    The coefficients indicate the extent of influence of each independent variable on the dependent variable.\n",
    "\n",
    "    It can be used for both explanatory (understanding variables’ influence) and predictive (forecasting new outputs) analytics problems.\n",
    "\n",
    "    Common metrics to evaluate the fit of the linear model include R-squared, RMSE, MAE, MAPE etc.\n",
    "\n",
    "    Regularization techniques like Ridge and Lasso can help address overfitting due to many features.\n",
    "\n",
    "    Many variations exist like Logistic Regression, Multi-variat Linear Regression, Polynomial Regression etc.\n",
    "\n",
    "So in summary, Linear Regression establishes a linear relationship between variables for prediction and explanation tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6017178b",
   "metadata": {},
   "source": [
    "# 2. Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08933d07",
   "metadata": {},
   "source": [
    "Multiple linear regression is an extension of simple linear regression to model the relationship between a scalar dependent variable and two or more explanatory variables.\n",
    "\n",
    "Here are a few key aspects about multiple regression:\n",
    "\n",
    "    It allows us to analyze the joint relationships and relative influence of multiple independent variables on a single dependent variable.\n",
    "\n",
    "    The regression equation becomes:\n",
    "\n",
    "y = b0 + b1x1 + b2x2 +…+ bnxn\n",
    "\n",
    "Where b0 is the intercept, b1 to bn are coefficients of variables x1 to xn.\n",
    "\n",
    "    Both continuous and categorical variables can be used as independents.\n",
    "\n",
    "    It helps determine the individual and partial contribution of each independent variable in explaining the variance in dependent variable.\n",
    "\n",
    "    Interpretation of coefficients is similar to simple linear regression – they indicate the expected change in Y with each one unit increase in the corresponding X, keeping others constant.\n",
    "\n",
    "    Evaluation metrics like R-squared, F-statistic, p-values, etc. are used to assess overall model fit and significance.\n",
    "\n",
    "    Assumptions include linearity, no multicollinearity, homoscedasticity, independence of errors, normality of residuals.\n",
    "\n",
    "    Variable selection techniques help determine the optimal set of predictors.\n",
    "\n",
    "So in summary, multiple regression extends linear modeling to multiple independent factors simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d22fa2",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c42ae",
   "metadata": {},
   "source": [
    "## 1.1 Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e0951",
   "metadata": {},
   "source": [
    "Key things to know about Ridge Regression:\n",
    "\n",
    "    It is a regularization technique used to address the problem of multicollinearity in linear regression models.\n",
    "\n",
    "    Multicollinearity occurs when independent variables are highly correlated, which increases the variance of the coefficient estimates.\n",
    "\n",
    "    Ridge adds a degree of “bias” to the coefficient estimates by imposing a penalty on the size of coefficients.\n",
    "\n",
    "    It works by adding the L2 norm (square) of the coefficients to the loss function that is being minimized during regression.\n",
    "\n",
    "    This shrinks the large coefficients and distributes the weight more evenly among correlated variables, improving generalization.\n",
    "\n",
    "    The shrinkage is controlled by a hyperparameter alpha. Higher alpha means more shrinkage of coefficients towards zero.\n",
    "\n",
    "    It helps avoid overfitting and gives more stable and reliable estimates compared to ordinary least squares regression.\n",
    "\n",
    "    Coefficients never become exactly zero but are shrunken, so all variables are retained in the model unlike LASSO.\n",
    "\n",
    "    Commonly used when there are many correlated predictors to get a stable set of predictors with predictive power.\n",
    "\n",
    "So in summary, Ridge applies L2 regularization to linear models by imposing a penalty on large coefficients to address multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64f86f",
   "metadata": {},
   "source": [
    "## 1.2 Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0474602",
   "metadata": {},
   "source": [
    "Key things to know about Lasso (Least Absolute Shrinkage and Selection Operator) Regression:\n",
    "\n",
    "    Like Ridge, it is also a regularization technique used for linear regression models.\n",
    "\n",
    "    However, instead of penalizing the L2 norm of coefficients like Ridge, Lasso penalizes the L1 norm (absolute sum).\n",
    "\n",
    "    This induces sparsity by forcing some coefficients to become exactly zero, automatically performing variable selection.\n",
    "\n",
    "    It selects a parsimonious model with fewer predictors than Ridge by driving unnecessary coefficients to zero.\n",
    "\n",
    "    Only the most informative predictors remain, ignoring least important ones and improving interpretability.\n",
    "\n",
    "    The degree of sparsity is controlled by the regularization hyperparameter (lambda).\n",
    "\n",
    "    Converges faster than Ridge as the cost function is convex but not necessarily differentiable.\n",
    "\n",
    "    Commonly used when the true underlying model is sparse in nature i.e. has only a few influential predictors.\n",
    "\n",
    "    Tends to give higher prediction accuracy than Ridge when number of features is very large as it selects only relevant features.\n",
    "\n",
    "So in summary, Lasso squeezes irrelevant coefficients to zero for simplified model interpretation while performing embedded feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea5fb9",
   "metadata": {},
   "source": [
    "## 1.3 Elastic Net Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20110b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Elastic Net Regression is a regularization technique that combines the approaches of Lasso and Ridge regression. Here are some key points about Elastic Net:\n",
    "\n",
    "    Like Lasso, it performs automatic variable selection by driving some coefficients to zero.\n",
    "\n",
    "    But similar to Ridge, it also handles groups of correlated predictors by combining both L1 and L2 penalties.\n",
    "\n",
    "    The regularization term is a linear combination of L1 and L2 norms: (1-α)L2 + αL1\n",
    "\n",
    "    α tunes the relative contribution of L1 vs L2 penalty between 0-1.\n",
    "\n",
    "    α=1 recovers Lasso, α=0 recovers Ridge regression.\n",
    "\n",
    "    It overcomes limitations of Lasso by allowing groups of correlated features to be selected together.\n",
    "\n",
    "    Performs better than Lasso in situations with highly correlated features.\n",
    "\n",
    "    The grouping effect makes coefficients more stable and parameter estimation consistent even with numerous predictors.\n",
    "\n",
    "    Useful as a compromise between sparsity of Lasso and grouping effect of Ridge regularization.\n",
    "\n",
    "    Hyperparameters like α and lambda need to be tuned for best performance.\n",
    "\n",
    "So in summary, Elastic Net achieves sparsity and grouping effect simultaneously, making it a flexible regression model for high-dimensional variable selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57db113",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfca634",
   "metadata": {},
   "source": [
    "Key points about logistic regression:\n",
    "\n",
    "    It is used when the dependent variable is categorical (binary or multiclass classification) rather than continuous.\n",
    "\n",
    "    It predicts the probability of an observation belonging to a specific class rather than the class itself.\n",
    "\n",
    "    The relationship between independent variables X and dependent variable Y is modelled using a logistic/sigmoid curve rather than linear function.\n",
    "\n",
    "    The output is a probability value between 0 and 1, via the logistic function formula:\n",
    "\n",
    "P(Y=1|X) = 1/(1+e^-(b0 + b1X1 + b2X2 +…))\n",
    "\n",
    "    The coefficients b have similar interpretations as linear regression – effect of one unit change in X on log odds of Y.\n",
    "\n",
    "    It can handle both continuous and categorical independents.\n",
    "\n",
    "    Can be extended for multiclass classification problems using techniques like one-vs-rest.\n",
    "\n",
    "    Evaluated using classification metrics like accuracy, AUC-ROC, confusion matrix etc. rather than R-squared.\n",
    "\n",
    "    Commonly used for problems like prediction, recommendation systems, sentiment analysis etc.\n",
    "\n",
    "So in summary, logistic regression applies when we want to model and predict categorical outcomes using linear regression techniques for multiple variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc183c3",
   "metadata": {},
   "source": [
    "## 3.1. R-sqrd for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e37b562",
   "metadata": {},
   "source": [
    "R-squared is not commonly used as an evaluation metric for logistic regression models because it works differently for linear and non-linear models. Here are some key points about R-squared in logistic regression:\n",
    "\n",
    "    In linear regression, R-squared represents the proportion of variance in the dependent variable that is explained by the independent variables. It ranges from 0 to 1.\n",
    "\n",
    "    However, for logistic regression the dependent variable is binary/categorical so the concept of variance doesn’t apply directly.\n",
    "\n",
    "    Logistic regression predicts probabilities rather than actual values, so the interpretation of R-squared is different.\n",
    "\n",
    "    Some variations of R-squared have been adapted for logistic regression but they do not have the same probabilistic interpretation as in linear models.\n",
    "\n",
    "    Pseudo R-squared metrics like Cox & Snell, Nagelkerke can range from 0 to 1 but may exceed 1 which is not desirable.\n",
    "\n",
    "    AUC (Area Under the ROC Curve) is a more robust metric to assess logistic regression performance as it doesn’t rely on assumptions behind R-squared.\n",
    "\n",
    "    Other classification metrics like accuracy, precision, recall are also more suitable than R-squared.\n",
    "\n",
    "So in summary, while R-squared can be calculated for logistic regression, it doesn’t have the same straightforward interpretation and probabilistic meaning as for linear regression models. Alternative metrics are preferred for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894baf00",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
